All right, this is the final test for whether or not my Chrome extension that transcribes the user's voice works. So how the system goes right now is that I have a Google Chrome extension with a transcribe button that will start listening for the user's voice through their microphone, and then after the user clicks on the stop transcribing button, it will send that file, that waveform file that it created, to a local server that I have hosted on my computer, which we can host later on on AWS or wherever we need to, which on that endpoint it sends that waveform file to be hosted on AWS's s three an s three bucket on AWS, which then what's it called? Which then gets a public facing URL to that file that was hosted on s three that automatically downloads it if you visit that link, just so the AssemblyAI can actually get the file, because otherwise it wouldn't be downloadable. Afterwards it will return that public facing URL to the extension, which will then start an API call to AssemblyAI to download the waveform file and queue it and start transcribing it. Then after we set a timeout for 3 seconds for a delay of 3 seconds so that we can actually let the AssemblyAI team or LLM transcribe our waveform file, which then after that delay of 3 seconds it will make another endpoint call to AssemblyAI to actually grab the text. And that is the end. And thank you for coming to my TED talk.